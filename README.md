How to run
=======

_python main.py -h_ for help

_python main.py --optim 'ARGD1'_ for different optimiser

_python main.py --lr '0.01'_ for different learning rates

_python main.py --optim ARGD1 --lr 0.000001_ to combine all

## Help
Please reffer to the help option for a more detailed look at the different parameters


## Current optimisers

1. ADADELTA
2. ADAGRAD
3. ADAMAX
4. ADAM
5. ARGD1
6. ASGD
7. LBFGS
8. LR_SCHEDULER
9. RMSPROP
10. RPROP
11. SGD
12. SPARSE_ADAM
